#!/usr/bin/env python3
"""
Test script for the comprehensive training pipeline

This script tests the training pipeline with optimized hyperparameters
to ensure all components work together correctly.
"""

import sys
import os
from pathlib import Path

# Add project root to path
sys.path.append('/home/Hudini/projects/info_spillover')

from src.training_pipeline import ComprehensiveTrainingPipeline
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_comprehensive_pipeline():
    """Test the comprehensive training pipeline"""

    print("üß™ TESTING COMPREHENSIVE TRAINING PIPELINE")
    print("="*60)

    try:
        # Initialize pipeline with optimized config
        config_path = "/home/Hudini/projects/info_spillover/experiments/configs/optimized_hyperparameters.yaml"

        pipeline = ComprehensiveTrainingPipeline(config_path)

        print("‚úÖ Pipeline initialization: PASSED")

        # Test individual components
        print("\n1Ô∏è‚É£ Testing MLflow setup...")
        pipeline.setup_mlflow_tracking()
        print("‚úÖ MLflow setup: PASSED")

        print("\n2Ô∏è‚É£ Testing data loading...")
        pipeline.load_data()
        print(f"‚úÖ Data loading: PASSED ({len(pipeline.raw_data)} records)")

        print("\n3Ô∏è‚É£ Testing data processing...")
        pipeline.process_data()
        print(f"‚úÖ Data processing: PASSED")
        print(f"   ‚Ä¢ Processed data: {pipeline.processed_data.shape}")
        print(f"   ‚Ä¢ Features: {pipeline.features_df.shape}")
        print(f"   ‚Ä¢ Network nodes: {pipeline.network.number_of_nodes()}")

        print("\n4Ô∏è‚É£ Testing statistical validation...")
        validation_report = pipeline.validate_statistical_assumptions()
        print(f"‚úÖ Statistical validation: PASSED")
        print(f"   ‚Ä¢ Tests performed: {validation_report['summary']['total_tests_performed']}")
        print(f"   ‚Ä¢ Pass rate: {validation_report['summary']['tests_passed'] / max(1, validation_report['summary']['total_tests_performed']):.2%}")

        print("\n5Ô∏è‚É£ Testing ML model training...")
        pipeline.train_ml_models()
        print(f"‚úÖ ML model training: PASSED ({len(pipeline.ml_models)} models)")
        for name, model_info in pipeline.ml_models.items():
            cv_score = model_info.get('cv_mean', 0)
            print(f"   ‚Ä¢ {name}: CV Score = {cv_score:.4f}")

        print("\n6Ô∏è‚É£ Testing spillover analysis...")
        pipeline.analyze_spillovers()
        if pipeline.spillover_results:
            total_spillover = pipeline.spillover_results.get('spillover_indices', {}).get('total_spillover_index', 0)
            print(f"‚úÖ Spillover analysis: PASSED (Total spillover = {total_spillover:.4f})")
        else:
            print("‚úÖ Spillover analysis: PASSED (No results - insufficient data)")

        print("\n7Ô∏è‚É£ Testing model evaluation...")
        pipeline.evaluate_models()
        print(f"‚úÖ Model evaluation: PASSED")
        for name, results in pipeline.ml_results.items():
            r2 = results.get('r2', 0)
            print(f"   ‚Ä¢ {name}: R¬≤ = {r2:.4f}")

        print("\n8Ô∏è‚É£ Testing results saving...")
        pipeline.save_results()
        print("‚úÖ Results saving: PASSED")

        print("\n" + "="*60)
        print("üéâ COMPREHENSIVE PIPELINE TEST COMPLETED!")
        print("="*60)
        print("‚úÖ All components working correctly")
        print("‚úÖ Optimized hyperparameters applied")
        print("‚úÖ Statistical validation integrated")
        print("‚úÖ MLflow tracking operational")
        print("‚úÖ Results saved successfully")

        return True

    except Exception as e:
        print(f"‚ùå Pipeline test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_quick_run():
    """Quick test run of the complete pipeline"""

    print("\nüöÄ RUNNING QUICK COMPLETE PIPELINE TEST")
    print("="*60)

    try:
        config_path = "/home/Hudini/projects/info_spillover/experiments/configs/optimized_hyperparameters.yaml"
        pipeline = ComprehensiveTrainingPipeline(config_path)

        # Run the complete pipeline
        pipeline.run_comprehensive_pipeline()

        return True

    except Exception as e:
        print(f"‚ùå Quick pipeline run failed: {e}")
        return False


if __name__ == "__main__":
    print("üß™ COMPREHENSIVE TRAINING PIPELINE TESTING")
    print("="*80)

    # Test individual components
    component_test_passed = test_comprehensive_pipeline()

    if component_test_passed:
        print(f"\n{'='*80}")
        print("üéØ COMPONENT TESTS PASSED - RUNNING FULL PIPELINE")
        print("="*80)

        # Test complete pipeline run
        full_pipeline_passed = test_quick_run()

        if full_pipeline_passed:
            print("\n" + "="*80)
            print("üèÜ ALL TESTS PASSED!")
            print("="*80)
            print("‚úÖ Training pipeline with optimized hyperparameters: WORKING")
            print("‚úÖ All components integrated successfully: WORKING")
            print("‚úÖ Statistical validation framework: WORKING")
            print("‚úÖ Optuna optimization: WORKING")
            print("‚úÖ MLflow tracking: WORKING")
            print("\nüöÄ Ready for production training!")
        else:
            print("\n‚ùå Full pipeline test failed")
    else:
        print("\n‚ùå Component tests failed")