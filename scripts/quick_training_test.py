#!/usr/bin/env python3
"""
Quick test of optimized training components

Tests core functionality without lengthy statistical validation.
"""

import sys
import os
import yaml
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
from sklearn.metrics import r2_score
import logging

# Add project root to path
sys.path.append('/home/Hudini/projects/info_spillover')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_optimized_hyperparameters():
    """Test ML models with optimized hyperparameters"""

    print("ğŸ¯ TESTING OPTIMIZED HYPERPARAMETERS")
    print("="*50)

    # Load optimized configuration
    config_path = "/home/Hudini/projects/info_spillover/experiments/configs/optimized_hyperparameters.yaml"

    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    print("âœ… Configuration loaded")

    # Generate test dataset
    np.random.seed(42)
    n_samples = 500

    # Create synthetic features with realistic patterns
    X = pd.DataFrame({
        'sentiment_compound': np.random.normal(0, 0.3, n_samples),
        'sentiment_positive': np.random.beta(2, 2, n_samples),
        'sentiment_negative': np.random.beta(2, 2, n_samples),
        'volume': np.random.exponential(1000, n_samples),
        'volatility': np.abs(np.random.normal(0.02, 0.01, n_samples)),
        'posts_count': np.random.poisson(20, n_samples),
        'price_change': np.random.normal(0, 0.05, n_samples)
    })

    # Create target with some relationship to features
    y = (X['sentiment_compound'] * 0.3 +
         X['sentiment_positive'] * 0.2 -
         X['sentiment_negative'] * 0.2 +
         np.random.normal(0, 0.1, n_samples))

    print(f"âœ… Generated dataset: {X.shape}")

    # Scale features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Test optimized models
    ml_config = config.get('ml_models', {})

    models_to_test = {
        'RandomForest': (RandomForestRegressor, ml_config.get('random_forest', {})),
        'GradientBoosting': (GradientBoostingRegressor, ml_config.get('gradient_boosting', {})),
        'Ridge': (Ridge, ml_config.get('ridge', {}))
    }

    results = {}

    for name, (model_class, params) in models_to_test.items():
        print(f"\nğŸ¤– Testing {name}...")
        print(f"   Parameters: {params}")

        try:
            # Initialize model with optimized parameters
            model = model_class(**params)

            # Cross-validation
            cv_scores = cross_val_score(model, X_scaled, y, cv=3, scoring='r2')

            # Fit and evaluate
            model.fit(X_scaled, y)
            predictions = model.predict(X_scaled)
            train_r2 = r2_score(y, predictions)

            results[name] = {
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'train_r2': train_r2,
                'params': params
            }

            print(f"   âœ… CV Score: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
            print(f"   âœ… Train RÂ²: {train_r2:.4f}")

        except Exception as e:
            print(f"   âŒ {name} failed: {e}")
            results[name] = {'error': str(e)}

    return results


def test_statistical_parameters():
    """Test statistical parameters from config"""

    print("\nğŸ“Š TESTING STATISTICAL PARAMETERS")
    print("="*50)

    config_path = "/home/Hudini/projects/info_spillover/experiments/configs/optimized_hyperparameters.yaml"

    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    stat_params = config.get('statistical_parameters', {})

    print("âœ… Optimized Statistical Parameters:")
    print(f"   â€¢ VAR max_lags: {stat_params.get('var_model', {}).get('max_lags', 15)}")
    print(f"   â€¢ Significance level: {stat_params.get('var_model', {}).get('significance_level', 0.012)}")
    print(f"   â€¢ Bootstrap iterations: {stat_params.get('bootstrap', {}).get('iterations', 1800)}")

    spillover_params = config.get('spillover_parameters', {})
    dy_params = spillover_params.get('diebold_yilmaz', {})

    print("âœ… Optimized Spillover Parameters:")
    print(f"   â€¢ Forecast horizon: {dy_params.get('forecast_horizon', 12)}")
    print(f"   â€¢ VAR lags: {dy_params.get('var_lags', 9)}")
    print(f"   â€¢ Rolling window: {dy_params.get('rolling_window', 65)}")

    return stat_params, spillover_params


def test_hierarchical_config():
    """Test hierarchical model configuration"""

    print("\nğŸ§  TESTING HIERARCHICAL MODEL CONFIG")
    print("="*50)

    config_path = "/home/Hudini/projects/info_spillover/experiments/configs/optimized_hyperparameters.yaml"

    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    hierarchical_config = config.get('hierarchical_models', {})

    print("âœ… LSTM Configuration:")
    lstm_config = hierarchical_config.get('subreddit_lstm', {})
    for param, value in lstm_config.items():
        print(f"   â€¢ {param}: {value}")

    print("âœ… Transformer Configuration:")
    transformer_config = hierarchical_config.get('subreddit_transformer', {})
    for param, value in transformer_config.items():
        print(f"   â€¢ {param}: {value}")

    print("âœ… GNN Configuration:")
    gnn_config = hierarchical_config.get('spillover_gnn', {})
    for param, value in gnn_config.items():
        print(f"   â€¢ {param}: {value}")

    return hierarchical_config


def main():
    """Run quick tests"""

    print("ğŸš€ QUICK TRAINING PIPELINE TEST")
    print("="*60)

    try:
        # Test ML models with optimized hyperparameters
        ml_results = test_optimized_hyperparameters()

        # Test statistical parameter configuration
        stat_params, spillover_params = test_statistical_parameters()

        # Test hierarchical model configuration
        hierarchical_config = test_hierarchical_config()

        # Print summary
        print("\n" + "="*60)
        print("ğŸ¯ TEST SUMMARY")
        print("="*60)

        print("ğŸ¤– ML Model Results:")
        best_model = ""
        best_score = -999

        for name, result in ml_results.items():
            if 'error' not in result:
                cv_score = result['cv_mean']
                print(f"   â€¢ {name}: CV = {cv_score:.4f}")

                if cv_score > best_score:
                    best_score = cv_score
                    best_model = name
            else:
                print(f"   â€¢ {name}: ERROR - {result['error']}")

        if best_model:
            print(f"\nğŸ† Best Model: {best_model} (CV = {best_score:.4f})")

        print(f"\nğŸ“Š Configuration Status:")
        print(f"   âœ… Optimized ML hyperparameters: LOADED")
        print(f"   âœ… Statistical parameters: LOADED")
        print(f"   âœ… Spillover parameters: LOADED")
        print(f"   âœ… Hierarchical model config: LOADED")

        print("\nğŸ‰ All optimized hyperparameters are properly configured!")
        print("ğŸš€ Ready for production training!")

    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()