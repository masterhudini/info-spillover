#!/usr/bin/env python3
"""
Lightweight pipeline runner - focus on data processing and BigQuery integration
Skips heavy ML dependencies (PyTorch, transformers) to test core functionality
"""

import os
import sys
import pandas as pd
import numpy as np
import json
import yaml
from pathlib import Path
from datetime import datetime, timedelta
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Add project root to path
sys.path.append('/home/Hudini/projects/info_spillover')

# Import our modules
from src.data.bigquery_client import BigQueryClient
from src.utils.gcp_setup import GCPAuthenticator


class LightweightPipeline:
    """Simplified pipeline focusing on BigQuery and basic ML"""

    def __init__(self):
        self.output_dir = Path("results/pipeline_light")
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Initialize MLflow
        mlflow.set_tracking_uri("sqlite:///mlflow_light.db")
        mlflow.set_experiment("info_spillover_light")

    def step_1_validate_setup(self):
        """Validate GCP and BigQuery setup"""
        print("=" * 60)
        print("STEP 1: VALIDATING SETUP")
        print("=" * 60)

        # Check BigQuery access
        if not GCPAuthenticator.test_bigquery_access():
            raise ConnectionError("BigQuery access failed")

        print("‚úÖ BigQuery connection validated")
        return True

    def step_2_generate_sample_data(self):
        """Generate comprehensive sample data for analysis"""
        print("=" * 60)
        print("STEP 2: GENERATING SAMPLE DATA")
        print("=" * 60)

        # Initialize BigQuery client
        bq_client = BigQueryClient(dataset_id="info_spillover_light_pipeline")

        # Create tables
        bq_client.create_posts_table()
        bq_client.create_prices_table()
        print("‚úÖ Tables created")

        # Generate sample Reddit data (more comprehensive)
        sample_data = self._generate_comprehensive_reddit_data()
        print(f"‚úÖ Generated {len(sample_data)} Reddit posts")

        # Generate sample price data (more comprehensive)
        sample_prices = self._generate_comprehensive_price_data()
        print(f"‚úÖ Generated {len(sample_prices)} price points")

        # Save and load to BigQuery
        temp_dir = Path("/tmp/pipeline_light_data")
        temp_dir.mkdir(exist_ok=True)

        # Save Reddit data
        with open(temp_dir / "sample_posts.json", 'w') as f:
            json.dump(sample_data, f, default=str)

        try:
            bq_client.load_json_data_to_bq(str(temp_dir))
            print("‚úÖ Reddit data loaded to BigQuery")
        except Exception as e:
            print(f"‚ö†Ô∏è  Reddit data loading issue: {e}")

        # Save price data
        sample_prices.to_csv(temp_dir / "btc-usd.csv", index=False)
        sample_prices.to_csv(temp_dir / "eth-usd.csv", index=False)

        bq_client.load_csv_data_to_bq(str(temp_dir))
        print("‚úÖ Price data loaded to BigQuery")

        return bq_client

    def step_3_data_analysis(self, bq_client):
        """Perform basic data analysis and feature engineering"""
        print("=" * 60)
        print("STEP 3: DATA ANALYSIS AND FEATURE ENGINEERING")
        print("=" * 60)

        # Get combined dataset
        combined_data = bq_client.create_combined_dataset(
            start_date="2023-01-01",
            end_date="2023-12-31"
        )

        print(f"‚úÖ Retrieved combined dataset: {combined_data.shape}")

        if combined_data.empty:
            print("‚ö†Ô∏è  No combined data available, using sentiment data only")
            combined_data = bq_client.get_post_sentiment_aggregation(
                start_date="2023-01-01",
                end_date="2023-12-31"
            )

        # Basic feature engineering
        if not combined_data.empty:
            # Add some simple features
            if 'avg_positive_sentiment' in combined_data.columns:
                combined_data['sentiment_momentum'] = combined_data.groupby('subreddit')['avg_positive_sentiment'].pct_change()
                combined_data['sentiment_volatility'] = combined_data.groupby('subreddit')['avg_positive_sentiment'].rolling(3).std().reset_index(0, drop=True)

            # Add time features
            combined_data['hour'] = combined_data.index.hour if hasattr(combined_data.index, 'hour') else 0
            combined_data['day_of_week'] = combined_data.index.dayofweek if hasattr(combined_data.index, 'dayofweek') else 0

            print(f"‚úÖ Feature engineering completed: {combined_data.shape}")

            # Save processed data
            combined_data.to_parquet(self.output_dir / "processed_data.parquet")
            print("‚úÖ Processed data saved")

        return combined_data

    def step_4_simple_modeling(self, data):
        """Simple machine learning model using scikit-learn"""
        print("=" * 60)
        print("STEP 4: SIMPLE MACHINE LEARNING MODEL")
        print("=" * 60)

        if data.empty:
            print("‚ö†Ô∏è  No data for modeling")
            return {}

        # Prepare data for modeling
        try:
            # Simple prediction task: predict sentiment based on time features
            feature_cols = ['hour', 'day_of_week', 'num_posts', 'num_comments']
            target_col = 'avg_positive_sentiment'

            # Filter available columns
            available_features = [col for col in feature_cols if col in data.columns]

            if not available_features or target_col not in data.columns:
                print("‚ö†Ô∏è  Insufficient features for modeling")
                return {}

            # Prepare dataset
            X = data[available_features].fillna(0)
            y = data[target_col].fillna(0)

            # Split data (simple time-based split)
            split_idx = int(len(X) * 0.8)
            X_train, X_test = X[:split_idx], X[split_idx:]
            y_train, y_test = y[:split_idx], y[split_idx:]

            print(f"‚úÖ Training set: {X_train.shape}, Test set: {X_test.shape}")

            # Start MLflow run
            with mlflow.start_run(run_name="simple_sentiment_model"):
                # Train simple model
                model = RandomForestRegressor(
                    n_estimators=50,
                    max_depth=10,
                    random_state=42
                )

                model.fit(X_train, y_train)

                # Predictions
                y_pred = model.predict(X_test)

                # Metrics
                mse = mean_squared_error(y_test, y_pred)
                r2 = r2_score(y_test, y_pred)

                # Log parameters and metrics
                mlflow.log_params({
                    "model_type": "RandomForest",
                    "n_estimators": 50,
                    "max_depth": 10,
                    "features": available_features
                })

                mlflow.log_metrics({
                    "mse": mse,
                    "r2_score": r2,
                    "train_samples": len(X_train),
                    "test_samples": len(X_test)
                })

                # Log model
                mlflow.sklearn.log_model(model, "sentiment_model")

                print(f"‚úÖ Model trained - R¬≤ Score: {r2:.3f}, MSE: {mse:.3f}")

                # Feature importance
                if hasattr(model, 'feature_importances_'):
                    importance = dict(zip(available_features, model.feature_importances_))
                    print("‚úÖ Feature importance:", importance)

                    # Save feature importance
                    importance_df = pd.DataFrame(
                        list(importance.items()),
                        columns=['feature', 'importance']
                    ).sort_values('importance', ascending=False)
                    importance_df.to_csv(self.output_dir / "feature_importance.csv", index=False)

                return {
                    "model": model,
                    "metrics": {"mse": mse, "r2": r2},
                    "feature_importance": importance if hasattr(model, 'feature_importances_') else {}
                }

        except Exception as e:
            print(f"‚ùå Modeling failed: {e}")
            return {}

    def step_5_basic_backtesting(self, data, model_results):
        """Simple backtesting simulation"""
        print("=" * 60)
        print("STEP 5: BASIC BACKTESTING SIMULATION")
        print("=" * 60)

        if data.empty or not model_results:
            print("‚ö†Ô∏è  No data or model for backtesting")
            return {}

        try:
            # Simple strategy: buy when sentiment is positive, sell when negative
            if 'avg_positive_sentiment' in data.columns:
                data['signal'] = np.where(data['avg_positive_sentiment'] > 0.6, 1,
                                        np.where(data['avg_positive_sentiment'] < 0.4, -1, 0))

                # Simple returns simulation (assuming we have price data)
                if 'price' in data.columns:
                    data['price_return'] = data.groupby(['symbol'])['price'].pct_change()
                    data['strategy_return'] = data['signal'].shift(1) * data['price_return']

                    # Calculate cumulative returns
                    cumulative_return = (1 + data['strategy_return'].fillna(0)).cumprod().iloc[-1] - 1
                    win_rate = (data['strategy_return'] > 0).mean()

                    backtest_results = {
                        "total_return": cumulative_return,
                        "win_rate": win_rate,
                        "total_trades": abs(data['signal'].diff()).sum() / 2,
                        "avg_return_per_trade": data['strategy_return'].mean()
                    }

                    print(f"‚úÖ Backtesting completed:")
                    print(f"   Total return: {cumulative_return:.2%}")
                    print(f"   Win rate: {win_rate:.2%}")
                    print(f"   Total trades: {backtest_results['total_trades']}")

                    return backtest_results

            else:
                print("‚ö†Ô∏è  No price data for backtesting")
                return {}

        except Exception as e:
            print(f"‚ùå Backtesting failed: {e}")
            return {}

    def step_6_generate_report(self, data, model_results, backtest_results):
        """Generate comprehensive report"""
        print("=" * 60)
        print("STEP 6: GENERATING REPORT")
        print("=" * 60)

        report = {
            "execution_timestamp": datetime.now().isoformat(),
            "data_summary": {
                "shape": data.shape if not data.empty else [0, 0],
                "columns": list(data.columns) if not data.empty else [],
                "date_range": {
                    "start": str(data.index.min()) if not data.empty and hasattr(data.index, 'min') else None,
                    "end": str(data.index.max()) if not data.empty and hasattr(data.index, 'max') else None
                }
            },
            "model_results": model_results,
            "backtest_results": backtest_results,
            "pipeline_version": "lightweight_v1.0"
        }

        # Save report
        with open(self.output_dir / "pipeline_report.json", 'w') as f:
            json.dump(report, f, indent=2, default=str)

        # Generate markdown report
        self._create_markdown_report(report)

        print("‚úÖ Report generated")
        return report

    def _generate_comprehensive_reddit_data(self):
        """Generate more comprehensive sample Reddit data"""
        subreddits = ['Bitcoin', 'ethereum', 'CryptoCurrency', 'dogecoin', 'litecoin']
        sample_data = []

        base_date = datetime(2023, 1, 1)

        for i in range(100):  # More posts
            post_date = base_date + timedelta(days=i // 4, hours=np.random.randint(0, 24))
            subreddit = np.random.choice(subreddits)

            post = {
                'subreddit': subreddit,
                'id': f'post_{i}',
                'title': f'{subreddit} discussion {i} - market analysis',
                'url': f'https://reddit.com/r/{subreddit}/post_{i}',
                'score': np.random.randint(1, 500),
                'text': f'Comprehensive analysis of {subreddit} market trends and sentiment {i}',
                'created_utc': int(post_date.timestamp()),
                'created_at': post_date.isoformat(),
                'num_comments': np.random.randint(5, 50),
                'comments': []
            }

            # Add comments with varied sentiment
            for j in range(np.random.randint(3, 15)):
                comment_date = post_date + timedelta(minutes=np.random.randint(1, 300))

                # Create sentiment distribution (more realistic)
                sentiment_categories = ['positive', 'negative', 'neutral']
                sentiment_weights = [0.4, 0.3, 0.3]  # Slightly positive bias
                sentiment_label = np.random.choice(sentiment_categories, p=sentiment_weights)

                if sentiment_label == 'positive':
                    sentiment_score = np.random.uniform(0.1, 1.0)
                elif sentiment_label == 'negative':
                    sentiment_score = np.random.uniform(0.1, 1.0)
                else:
                    sentiment_score = np.random.uniform(0.0, 0.3)

                comment = {
                    'id': f'comment_{i}_{j}',
                    'author': f'crypto_user_{np.random.randint(1000, 9999)}',
                    'body': f'{sentiment_label.capitalize()} sentiment comment {j} about {subreddit}',
                    'score': np.random.randint(1, 100),
                    'created_utc': int(comment_date.timestamp()),
                    'created_at': comment_date.isoformat(),
                    'sentiment': {
                        'label': sentiment_label,
                        'score': sentiment_score
                    }
                }
                post['comments'].append(comment)

            sample_data.append(post)

        return sample_data

    def _generate_comprehensive_price_data(self):
        """Generate more realistic cryptocurrency price data"""
        base_date = datetime(2023, 1, 1)
        dates = pd.date_range(start=base_date, periods=100, freq='D')

        price_data = []

        # BTC data
        current_btc_price = 30000
        for date in dates:
            volatility = np.random.normal(0.0, 0.03)  # 3% daily volatility
            current_btc_price *= (1 + volatility)

            price_data.append({
                'snapped_at': date,
                'price': current_btc_price,
                'market_cap': current_btc_price * 19_500_000,  # BTC supply
                'total_volume': current_btc_price * np.random.uniform(500000, 2000000)
            })

        return pd.DataFrame(price_data)

    def _create_markdown_report(self, report):
        """Create markdown report"""
        markdown_content = f"""
# Lightweight Information Spillover Analysis

**Generated:** {report['execution_timestamp']}

## Data Summary
- **Dataset Shape:** {report['data_summary']['shape'][0]:,} rows √ó {report['data_summary']['shape'][1]} columns
- **Columns:** {len(report['data_summary']['columns'])} features
- **Date Range:** {report['data_summary']['date_range']['start']} to {report['data_summary']['date_range']['end']}

## Model Performance
"""

        if report['model_results']:
            metrics = report['model_results']['metrics']
            markdown_content += f"""
- **R¬≤ Score:** {metrics['r2']:.3f}
- **MSE:** {metrics['mse']:.3f}
- **Model Type:** Random Forest Regressor

### Feature Importance
"""
            if 'feature_importance' in report['model_results']:
                for feature, importance in report['model_results']['feature_importance'].items():
                    markdown_content += f"- **{feature}:** {importance:.3f}\n"

        if report['backtest_results']:
            backtest = report['backtest_results']
            markdown_content += f"""

## Backtesting Results
- **Total Return:** {backtest.get('total_return', 0):.2%}
- **Win Rate:** {backtest.get('win_rate', 0):.2%}
- **Total Trades:** {backtest.get('total_trades', 0)}
- **Average Return per Trade:** {backtest.get('avg_return_per_trade', 0):.4f}
"""

        markdown_content += """

## Pipeline Status
‚úÖ **BigQuery Integration:** Working
‚úÖ **Data Processing:** Complete
‚úÖ **Machine Learning:** Simple model trained
‚úÖ **Backtesting:** Basic simulation complete

---
*Generated by Lightweight Information Spillover Pipeline v1.0*
"""

        with open(self.output_dir / "report.md", 'w') as f:
            f.write(markdown_content)

    def run_complete_pipeline(self):
        """Execute the complete lightweight pipeline"""
        print("üöÄ STARTING LIGHTWEIGHT INFORMATION SPILLOVER PIPELINE")
        print("=" * 80)

        start_time = datetime.now()

        try:
            # Run all steps
            self.step_1_validate_setup()
            bq_client = self.step_2_generate_sample_data()
            data = self.step_3_data_analysis(bq_client)
            model_results = self.step_4_simple_modeling(data)
            backtest_results = self.step_5_basic_backtesting(data, model_results)
            report = self.step_6_generate_report(data, model_results, backtest_results)

            end_time = datetime.now()
            total_time = end_time - start_time

            print("=" * 80)
            print("üéâ LIGHTWEIGHT PIPELINE COMPLETED SUCCESSFULLY!")
            print(f"‚è±Ô∏è  Total execution time: {total_time}")
            print(f"üìä Data processed: {data.shape if not data.empty else 'No data'}")
            print(f"üíæ Results saved to: {self.output_dir}")
            print("=" * 80)

            return {
                'success': True,
                'execution_time': total_time,
                'data_shape': data.shape if not data.empty else [0, 0],
                'output_dir': str(self.output_dir),
                'report': report
            }

        except Exception as e:
            print(f"‚ùå Pipeline failed: {e}")
            print("Check the logs above for detailed error information")
            return {
                'success': False,
                'error': str(e),
                'execution_time': datetime.now() - start_time
            }


def main():
    """Main function"""
    pipeline = LightweightPipeline()
    results = pipeline.run_complete_pipeline()

    if results['success']:
        print(f"\n‚úÖ Pipeline completed in {results['execution_time']}")
        return True
    else:
        print(f"\n‚ùå Pipeline failed: {results['error']}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)