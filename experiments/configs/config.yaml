# Information Spillover Experiment Configuration

experiment:
  name: "info_spillover_crypto_sentiment"
  description: "Cryptocurrency price prediction using Reddit sentiment analysis"
  tags:
    - "crypto"
    - "sentiment"
    - "price_prediction"
  random_seed: 42

# Data configuration
data:
  start_date: "2021-01-01"
  end_date: "2023-12-31"
  test_size: 0.2
  val_size: 0.25
  random_state: 42
  # Hierarchical model parameters
  sequence_length: 24
  prediction_horizon: 1
  scaling_method: "standard"

# Model configuration
model:
  type: "random_forest"  # Options: random_forest, logistic_regression, svm
  params:
    n_estimators: 100
    max_depth: 10
    min_samples_split: 5
    min_samples_leaf: 2
    random_state: 42
    class_weight: "balanced"  # Handle class imbalance
    n_jobs: -1

# Feature engineering configuration
features:
  sentiment_features:
    - avg_positive_sentiment
    - avg_negative_sentiment
    - avg_neutral_sentiment
    - positive_ratio
    - negative_ratio
    - neutral_ratio
    - sentiment_score

  activity_features:
    - num_posts
    - num_comments
    - avg_post_score
    - avg_comment_score
    - activity_score
    - engagement_score

  price_features:
    - price
    - market_cap
    - total_volume
    - price_change_pct

# Training configuration
training:
  cross_validation: true
  cv_folds: 5
  metrics:
    - accuracy
    - precision
    - recall
    - f1_score

# BigQuery configuration
bigquery:
  project_id: null  # Will use default from environment
  dataset_id: "info_spillover"
  location: "US"

# MLFlow configuration
mlflow:
  experiment_name: "info_spillover_experiment"
  tracking_uri: "sqlite:///mlflow.db"
  artifact_location: "experiments/outputs/mlruns"

# Hierarchical Models Configuration
hierarchical_models:
  subreddit_model_type: "lstm"  # Options: lstm, transformer
  hidden_dim: 128
  num_layers: 2
  dropout: 0.2
  attention: true
  d_model: 128
  nhead: 8
  num_encoder_layers: 3
  gnn_hidden_dim: 64
  gnn_num_layers: 3
  gnn_type: "GAT"  # Options: GAT, GCN, GGNN
  learning_rate: 0.001
  weight_decay: 0.0001
  batch_size: 32
  training:
    max_epochs: 50
    patience: 10

# Machine Learning Models Configuration
ml_models:
  random_forest:
    n_estimators: 100
    max_depth: 10
    min_samples_split: 5
    min_samples_leaf: 2
    random_state: 42
    n_jobs: -1
  gradient_boosting:
    n_estimators: 100
    max_depth: 6
    learning_rate: 0.1
    random_state: 42
  ridge:
    alpha: 1.0
    random_state: 42
  svm:
    C: 1.0
    gamma: 'scale'
    kernel: 'rbf'

# Cross Validation Configuration
cross_validation:
  n_splits: 5
  shuffle: false  # Use TimeSeriesSplit for temporal data

# Reproducibility Configuration
reproducibility:
  set_seeds: true

# Output paths
outputs:
  models: "models/saved"
  plots: "experiments/outputs/plots"
  metrics: "experiments/outputs"
  logs: "experiments/logs"